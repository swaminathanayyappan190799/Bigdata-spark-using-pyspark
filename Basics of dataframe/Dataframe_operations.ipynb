{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvhjFsgQYXCi"
      },
      "source": [
        "# **Operations performed in spark dataframe**\n",
        "\n",
        "Once if you load your data as a dataframe using pyspark, there are lot many operations available in spark dataframe which will be more useful in analyzing the dara in a better way. In this notebook let's discuss how to\n",
        "\n",
        "\n",
        "\n",
        "> Execute SQL commands in a spark dataframe.\n",
        "\n",
        "\n",
        "> Query the spark dataframe using filter with single and multiple conditional operators.\n",
        "\n",
        "\n",
        "> Converting a query result (spark datframe) into a json(python dictionary).\n",
        "\n",
        "\n",
        "> Groupby,Orderby and aggregate functions\n",
        "\n",
        "> Filling and handling null/empty values\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tlFBOacIWRRh"
      },
      "outputs": [],
      "source": [
        "#As an initial step let's create a spark session.\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"df_ops\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VymNV7RfZksF"
      },
      "outputs": [],
      "source": [
        "#Reading the data using spark as a dataframe.\n",
        "df=spark.read.csv(\"Data1.csv\", inferSchema=True, header=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBLdeWejaGWh",
        "outputId": "ebfd9530-2225-4ab5-c596-b8eab0fcbbfa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+----+\n",
            "|       Name| Age|\n",
            "+-----------+----+\n",
            "|Swaminathan|  24|\n",
            "|      Peter|null|\n",
            "|        Sam|  54|\n",
            "|      Henry|null|\n",
            "+-----------+----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RAvcQXxiaJ18",
        "outputId": "4bd4d591-a509-4e39-9e65-bd23cc29a193"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+----+------+\n",
            "|       Name| Age|NewAge|\n",
            "+-----------+----+------+\n",
            "|Swaminathan|  24|    48|\n",
            "|      Peter|null|  null|\n",
            "|        Sam|  54|   108|\n",
            "|      Henry|null|  null|\n",
            "+-----------+----+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Creating one new column as \"NewAge\" in the dataframe.\n",
        "df=df.withColumn(\"NewAge\",df[\"Age\"]*2)\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzI7bOGLahXm"
      },
      "source": [
        "## **Executing SQL commands on spark dataframe**\n",
        "\n",
        "Querying using SQL commands over a spark dataframe is possible by creating a temporary view of the entire dataframe as shown below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "IwxXyF3aaW0L"
      },
      "outputs": [],
      "source": [
        "# I have created a temporary view by replicating the entire spark dataframe in the name of \"employee.\"\n",
        "df.createOrReplaceTempView(\"employee\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHZL1dAXaqMR"
      },
      "source": [
        "Getting data of a employee whose age is less than or equal to 25."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjru35Q_anSe",
        "outputId": "c865dbb0-1a09-482c-e7a6-cc9ae99b3754"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+---+------+\n",
            "|       Name|Age|NewAge|\n",
            "+-----------+---+------+\n",
            "|Swaminathan| 24|    48|\n",
            "+-----------+---+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Getting Employee's age less than or equal to 25\n",
        "spark.sql(\"SELECT * FROM employee WHERE Age<=25\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1497rhbYa15I"
      },
      "source": [
        "Getting name of the employee whose age is beyond 50."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aF5UAbpUaysO",
        "outputId": "51c10255-3026-4ead-de94-8b31ffd3d851"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+\n",
            "|Name|\n",
            "+----+\n",
            "| Sam|\n",
            "+----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Getting name of the employee whose age is beyond 50.\n",
        "spark.sql(\"SELECT Name FROM employee WHERE Age>=50\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIU1wojsbOh7"
      },
      "source": [
        "Getting name of the person whose age is 24 and newage is 48"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k96GOMiobKic",
        "outputId": "26648111-3b97-4cc9-9085-24699e5e2b48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+\n",
            "|       Name|\n",
            "+-----------+\n",
            "|Swaminathan|\n",
            "+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Getting name of the employee whose age is 24 and newage is 48.\n",
        "spark.sql(\"SELECT Name FROM employee WHERE Age==24 AND NewAge==48\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8cpcKUfb7au"
      },
      "source": [
        "## **Query the spark dataframe using filter**\n",
        "\n",
        "As like executing SQL queries in a spark dataframe, we can also query a spark dataframe using the filter() that is available for pyspark dataframes. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfpwoI2Sc_b1"
      },
      "source": [
        "### **Filtering using single condition**\n",
        "\n",
        "Let's see how to filter a spark dataframe using one condition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkLBcSB5bjmM",
        "outputId": "8ea6915e-5854-46ec-afa7-81f7854165e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+---+------+\n",
            "|Name|Age|NewAge|\n",
            "+----+---+------+\n",
            "| Sam| 54|   108|\n",
            "+----+---+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Getting details of a employee whose age is above 50.\n",
        "df.filter(\"Age > 50\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXXreBfndjTK"
      },
      "source": [
        "You can also do the same query in another way as shown below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1TF3CFABdab4",
        "outputId": "37ae016f-c0ed-495a-f15d-f07eadfdd1a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+---+------+\n",
            "|Name|Age|NewAge|\n",
            "+----+---+------+\n",
            "| Sam| 54|   108|\n",
            "+----+---+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.filter(df[\"Age\"]>50).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogWKhyzDdril",
        "outputId": "a3f3b07f-4178-43dc-e95c-ab7226e56477"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+---+\n",
            "|Name|Age|\n",
            "+----+---+\n",
            "| Sam| 54|\n",
            "+----+---+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Let's choose two columns out from the filter result using select()\n",
        "df.filter(df[\"Age\"]>50).select([\"Name\",\"Age\"]).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTOji1x3eBXE"
      },
      "source": [
        "### **Filtering using multiple conditions**\n",
        "\n",
        "Basically you can query a datframe with one or more than one conditions using filter(), those multiple conditions will be declared using the logical operators like  (AND)& , (OR)|, (NOT)~ . Spark dataframe filter supports logical operations declared only in the symbol rather than declaring those operators in words, and also the conditions should be enclosed within the brackets (). If you don't do so then it may throws an error. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UgITAqFed-C6",
        "outputId": "da55a16a-15e6-4812-f5a9-4a736117b5a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+---+------+\n",
            "|       Name|Age|NewAge|\n",
            "+-----------+---+------+\n",
            "|Swaminathan| 24|    48|\n",
            "+-----------+---+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Filtering using multiple condition\n",
        "df.filter((df[\"Age\"]>20) & (df[\"Name\"]==\"Swaminathan\")).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Em9C5ykafVzK",
        "outputId": "6ca6d336-44db-495c-dcd2-d6e1eed846ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+---+\n",
            "|       Name|Age|\n",
            "+-----------+---+\n",
            "|Swaminathan| 24|\n",
            "+-----------+---+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Choosing specific columns of a filtered spark dataframe\n",
        "df.filter((df[\"Age\"]>20) & (df[\"Name\"]==\"Swaminathan\")).select([\"Name\",\"Age\"]).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ya7I0tNf5Me"
      },
      "source": [
        "## **Converting a filter query result to json(python dictionary)**\n",
        "\n",
        "You can convert a filtered query result which is basically in the form of a spark dataframe into a python dictionary(json) using the collect() and asDict() method available in pyspark and python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLF-461Pfu_f",
        "outputId": "fb00aa91-d944-4ed7-d779-6315a749f0fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+---+------+\n",
            "|Name|Age|NewAge|\n",
            "+----+---+------+\n",
            "| Sam| 54|   108|\n",
            "+----+---+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Let's filter the record of a individual whose new age is beyond 60.\n",
        "df.filter(df[\"NewAge\"]>60).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "NBOSrw1Ygc-4"
      },
      "outputs": [],
      "source": [
        "#Later then let's assign the filter result to a variable\n",
        "filter_result = df.filter(df[\"NewAge\"]>60).collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aainAb7mhB8z",
        "outputId": "7954afea-b1e7-4799-9986-1e21d94fa90c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Row(Name='Sam', Age=54, NewAge=108)]"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#The result would be a list of rows.\n",
        "filter_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DiiUjHHnkE3T",
        "outputId": "e1128f4c-3c67-4ef0-be45-d110be388bdd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'Name': 'Sam', 'Age': 54, 'NewAge': 108}"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Let's convert the result into a python dictionary\n",
        "filter_result[0].asDict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "z-X8s9ENkSGJ",
        "outputId": "6147646d-e765-4388-f47f-c457e9ce2c43"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Sam'"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Let's try to get the \"Name\" from the dictionary\n",
        "#Let's convert the result into a python dictionary\n",
        "filter_result[0].asDict()[\"Name\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mhqKX8gKE0_"
      },
      "source": [
        "## **Group By, Orderby and aggregate functions**\n",
        "\n",
        "  These functions will be useful in combining multiple simlar rows/records based on some conditions.This way you can explore the data in a better way. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "1J_ieAPuJ773"
      },
      "outputs": [],
      "source": [
        "#Loading the car_sales data.\n",
        "df = spark.read.csv(\"car_sales.csv\",inferSchema=True,header=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgE_xiUDK5qa"
      },
      "source": [
        "Note:This is a custom created data for this demo purpose, this does not depict the original sales of any car companies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AiuyBSzkMfqg",
        "outputId": "79755487-13af-46e2-b324-fa38250bbd8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- Brand: string (nullable = true)\n",
            " |-- Price: integer (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Pq5EtSrKw-v",
        "outputId": "508c68e6-5194-4208-9662-476171c6696e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------------+-------+\n",
            "|             Brand|  Price|\n",
            "+------------------+-------+\n",
            "|              BENZ|4500000|\n",
            "|               BMW|6000000|\n",
            "|              AUDI|6400000|\n",
            "|              FORD|5345532|\n",
            "|               BMW|2345452|\n",
            "|              BENZ|3564577|\n",
            "|              AUDI|5453453|\n",
            "|              FORD|7854085|\n",
            "|Morris and Garages|8957405|\n",
            "|             SKODA|6859358|\n",
            "|              JEEP|4309584|\n",
            "|        VOLKSWAGEN|5487504|\n",
            "|              BENZ|8465047|\n",
            "|Morris and Garages|6398535|\n",
            "|        VOLKSWAGEN|3247504|\n",
            "|              JEEP|6437453|\n",
            "+------------------+-------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgjqXXh4LG9X"
      },
      "source": [
        "### **Group by**\n",
        "\n",
        "Group by will helps you to combine similar row entries with a standard operation like(sum,mean) etc,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hIm4MEBBK0lP",
        "outputId": "6f2f39dd-8e21-414a-b30a-1d1b636f9279"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------------+-----------------+\n",
            "|             Brand|       avg(Price)|\n",
            "+------------------+-----------------+\n",
            "|             SKODA|        6859358.0|\n",
            "|              AUDI|        5926726.5|\n",
            "|              FORD|        6599808.5|\n",
            "|Morris and Garages|        7677970.0|\n",
            "|              JEEP|        5373518.5|\n",
            "|               BMW|        4172726.0|\n",
            "|        VOLKSWAGEN|        4367504.0|\n",
            "|              BENZ|5509874.666666667|\n",
            "+------------------+-----------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Making a groupby of car company names by making a average of their sales value.\n",
        "df.groupBy(\"Brand\").mean().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJi3n6nfLqiv",
        "outputId": "6b28d0cc-5f1a-400d-f999-dda716e12417"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------------+-----+\n",
            "|             Brand|count|\n",
            "+------------------+-----+\n",
            "|             SKODA|    1|\n",
            "|              AUDI|    2|\n",
            "|              FORD|    2|\n",
            "|Morris and Garages|    2|\n",
            "|              JEEP|    2|\n",
            "|               BMW|    2|\n",
            "|        VOLKSWAGEN|    2|\n",
            "|              BENZ|    3|\n",
            "+------------------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Making a groupby of car company names by making a count of their each occorence.\n",
        "df.groupBy(\"Brand\").count().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-md1YD5yM_9y"
      },
      "source": [
        "### **Aggregate functions**\n",
        "\n",
        "Aggregate function is more same to groupby but it will only displays the certain column value instead of displaying the entire pyspark dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmg4lUi-MKvb",
        "outputId": "5ab3d651-7f79-4d1d-d41e-fc58b627c2dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+\n",
            "|sum(Price)|\n",
            "+----------+\n",
            "|  91625489|\n",
            "+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Lets aggregate the sum of sales made by all the car companies\n",
        "df.agg({\"Price\":\"sum\"}).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jcSOtit2Nky_",
        "outputId": "e02fcb45-fbb3-4f6b-f4b6-d27ca88470b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------+\n",
            "|  avg(Price)|\n",
            "+------------+\n",
            "|5726593.0625|\n",
            "+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Lets aggregate the average of sales made by all the car companies\n",
        "df.agg({\"Price\":\"avg\"}).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWX8JhF7NuwE",
        "outputId": "91f94b3b-feca-4cbc-fb74-41c6404b3f96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+\n",
            "|max(Price)|\n",
            "+----------+\n",
            "|   8957405|\n",
            "+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Lets aggregate the maximum of sales made by the car company\n",
        "df.agg({\"Price\":\"max\"}).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FCdrIiw7YaH0",
        "outputId": "ead7903c-08c5-40f8-86d3-1384282dc808"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------+\n",
            "|count(Brand)|\n",
            "+------------+\n",
            "|          16|\n",
            "+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.agg({\"Brand\":\"count\"}).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFreYhW6Ylfq"
      },
      "source": [
        "### **Executing spark functions using aggregate**\n",
        "\n",
        "There are some available functions in pyspark such as stddev,mean,count_distinct that used to make operations in a spark dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "t2udzeTPZWKN"
      },
      "outputs": [],
      "source": [
        "#Importing spark dataframe functions like standard deviaton,mean and getting count of distinct values.\n",
        "from pyspark.sql.functions import stddev, mean, count_distinct, format_number"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVegjpguZWG4",
        "outputId": "8536b764-2768-4113-e16e-9a2863075aca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------------------------+\n",
            "|Standard deviation of price|\n",
            "+---------------------------+\n",
            "|               1,848,845.46|\n",
            "+---------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Getting standard deviation of price using spark function.\n",
        "#alias() will let you name the aggregated column instead of the default name.\n",
        "#format_number() will reduces the decimal positions of the calculated value.\n",
        "df.select(format_number(stddev(\"Price\"),2).alias(\"Standard deviation of price\")).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UowqrEpWZWBh",
        "outputId": "6e8e49dc-1983-4b41-b65d-f71a176509f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------------+\n",
            "|Average of price|\n",
            "+----------------+\n",
            "|    5726593.0625|\n",
            "+----------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Getting mean(average) of price using spark function.\n",
        "df.select(mean(\"Price\").alias(\"Average of price\")).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5DqVASekZVzX",
        "outputId": "cb73744f-aa95-4974-b662-b7128e9eebde"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------+\n",
            "|Unique brand|\n",
            "+------------+\n",
            "|           8|\n",
            "+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Getting counts of unique brand names using spark function.\n",
        "df.select(count_distinct(\"Brand\").alias(\"Unique brand\")).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrbAHdX5cZpc"
      },
      "source": [
        "### **Order by**\n",
        "\n",
        "The orderby functionality is used to arrange the ro w elements in ascending and descending order "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yAgOgz5FOAMK",
        "outputId": "369ae561-ab62-459b-f643-aa39b32d0059"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------------+-------+\n",
            "|             Brand|  Price|\n",
            "+------------------+-------+\n",
            "|              AUDI|6400000|\n",
            "|              AUDI|5453453|\n",
            "|              BENZ|3564577|\n",
            "|              BENZ|8465047|\n",
            "|              BENZ|4500000|\n",
            "|               BMW|6000000|\n",
            "|               BMW|2345452|\n",
            "|              FORD|5345532|\n",
            "|              FORD|7854085|\n",
            "|              JEEP|6437453|\n",
            "|              JEEP|4309584|\n",
            "|Morris and Garages|8957405|\n",
            "|Morris and Garages|6398535|\n",
            "|             SKODA|6859358|\n",
            "|        VOLKSWAGEN|5487504|\n",
            "|        VOLKSWAGEN|3247504|\n",
            "+------------------+-------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Ordering brand names using order by\n",
        "#By default it is in ascending order\n",
        "df.orderBy(\"Brand\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILi_K5eTOUsY",
        "outputId": "e8ec79a5-7868-4ec7-9996-aff56fc948ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------------+-------+\n",
            "|             Brand|  Price|\n",
            "+------------------+-------+\n",
            "|        VOLKSWAGEN|5487504|\n",
            "|        VOLKSWAGEN|3247504|\n",
            "|             SKODA|6859358|\n",
            "|Morris and Garages|6398535|\n",
            "|Morris and Garages|8957405|\n",
            "|              JEEP|4309584|\n",
            "|              JEEP|6437453|\n",
            "|              FORD|5345532|\n",
            "|              FORD|7854085|\n",
            "|               BMW|2345452|\n",
            "|               BMW|6000000|\n",
            "|              BENZ|4500000|\n",
            "|              BENZ|3564577|\n",
            "|              BENZ|8465047|\n",
            "|              AUDI|6400000|\n",
            "|              AUDI|5453453|\n",
            "+------------------+-------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Ordering brand names by descending order\n",
        "df.orderBy(\"Brand\",ascending=False).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dOS3LlhdrSd",
        "outputId": "7a55f7b1-5822-44d9-b313-ba61446ce113"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------------+----------+\n",
            "|             Brand|sum(Price)|\n",
            "+------------------+----------+\n",
            "|              AUDI|  11853453|\n",
            "|              BENZ|  16529624|\n",
            "|               BMW|   8345452|\n",
            "|              FORD|  13199617|\n",
            "|              JEEP|  10747037|\n",
            "|Morris and Garages|  15355940|\n",
            "|             SKODA|   6859358|\n",
            "|        VOLKSWAGEN|   8735008|\n",
            "+------------------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Using group and order by together\n",
        "df.groupBy(\"Brand\").sum().orderBy(\"Brand\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTRWJ_oyslpV"
      },
      "source": [
        "## **Filling/handling null and empty values**\n",
        "\n",
        "Whenever if you have data, there will be more chance of occurence for null or empty row values on some parts of the dataframe. Basically this leads to information redundancy and inorder to overcome this let's see how to handle the null values in a spark dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "uzgJL0ZAslpV"
      },
      "outputs": [],
      "source": [
        "#Loading the dataset.\n",
        "df = spark.read.csv(\"Data1.csv\", inferSchema=True, header=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "G-4m2a3bslpV",
        "outputId": "13f031f7-534b-4970-c806-c9b55c2c1180"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+----+\n",
            "|       Name| Age|\n",
            "+-----------+----+\n",
            "|Swaminathan|  24|\n",
            "|      Peter|null|\n",
            "|        Sam|  54|\n",
            "|      Henry|null|\n",
            "+-----------+----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EeIMojY4slpV"
      },
      "source": [
        "In the above dataset we have certain rows of the age column has null values, let's try to remove/resolve that using pyspark builtin methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "hvpBJCirslpW",
        "outputId": "c6b77934-160f-459e-8ee6-9e01ec63cdf0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+---+\n",
            "|       Name|Age|\n",
            "+-----------+---+\n",
            "|Swaminathan| 24|\n",
            "|        Sam| 54|\n",
            "+-----------+---+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Dropping null values\n",
        "#Will generally remove the entire row which has null value for a column\n",
        "df.na.drop().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "RlJBo-e7slpW",
        "outputId": "2b607bd3-e440-4209-cea7-4afdf40f28aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+----+\n",
            "|       Name| Age|\n",
            "+-----------+----+\n",
            "|Swaminathan|  24|\n",
            "|      Peter|null|\n",
            "|        Sam|  54|\n",
            "|      Henry|null|\n",
            "+-----------+----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Setting threshold for dropping null\n",
        "#Threshold means the minimum amount of null values that should be there for a specific row\n",
        "#We get the full dataframe here because we only have two columns.\n",
        "df.na.drop(thresh=1).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GynNNTIbslpW"
      },
      "source": [
        "#### **Dropping null values based on methods**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "epsa0S8IslpW",
        "outputId": "913d0cbc-097b-4380-e4d3-b79b0d78f61a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+---+\n",
            "|       Name|Age|\n",
            "+-----------+---+\n",
            "|Swaminathan| 24|\n",
            "|        Sam| 54|\n",
            "+-----------+---+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#Drops any null values within the dataframe\n",
        "df.na.drop(how=\"any\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "aoWxhT7LslpW",
        "outputId": "0c9f577b-f1fa-42dc-d275-92f744cf0cae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+----+\n",
            "|       Name| Age|\n",
            "+-----------+----+\n",
            "|Swaminathan|  24|\n",
            "|      Peter|null|\n",
            "|        Sam|  54|\n",
            "|      Henry|null|\n",
            "+-----------+----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Drops all the null values in the datframe.\n",
        "df.na.drop(how=\"all\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atTnih5UslpW"
      },
      "source": [
        "#### **Creating subset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "ZcMTrxvuslpW",
        "outputId": "b570dae9-8150-475e-e492-c701d4d941f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+---+\n",
            "|       Name|Age|\n",
            "+-----------+---+\n",
            "|Swaminathan| 24|\n",
            "|        Sam| 54|\n",
            "+-----------+---+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Subset will gives the dataframe without the null for the column that is mentioned on this function parameter.\n",
        "df.na.drop(subset=\"Age\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fC_GxNUAslpW"
      },
      "source": [
        "#### **Filling null values**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "lUXpXL_aslpW",
        "outputId": "5d89d2c7-c86b-4846-fdaf-62bc78b24e18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- Name: string (nullable = true)\n",
            " |-- Age: integer (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Filling null values in the Age column.\n",
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Tyw52A0slpX"
      },
      "source": [
        "As you see the dataframe has two columns one is of type string and another one is of type integer, if you give any string value for the replacement of null value then it will replace the null value in the column that was in string type. Likewise if you are providing any number values, it will fill the null value for the column that belongs to integer type."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "TArvpaTCslpX",
        "outputId": "b583e119-63ef-48ee-c8a6-099ed2ebb3f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+---+\n",
            "|       Name|Age|\n",
            "+-----------+---+\n",
            "|Swaminathan| 24|\n",
            "|      Peter| 20|\n",
            "|        Sam| 54|\n",
            "|      Henry| 20|\n",
            "+-----------+---+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Let's fill the null values in the Age column\n",
        "df.na.fill(20).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "2-N5N81aslpX",
        "outputId": "2feeadf9-29c2-4651-d593-cf7bcfb66a38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+----+\n",
            "|       Name| Age|\n",
            "+-----------+----+\n",
            "|Swaminathan|  24|\n",
            "|      Peter|null|\n",
            "|        Sam|  54|\n",
            "|      Henry|null|\n",
            "+-----------+----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Let's check the same function by giving a string value.\n",
        "df.na.fill(\"test_value\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FuZAgPcslpX"
      },
      "source": [
        "Here you see the null values in the Age column is not replaced since Age is a integer type based column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "Vs4kfEyRslpX",
        "outputId": "ce145ac1-1a9a-4bf6-cb5d-9f5720a39339"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "39.0"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Let's fill the null values of the age column with it's mean using the spark functions\n",
        "from pyspark.sql.functions import mean\n",
        "mean_age=df.select(mean(\"Age\").alias(\"Meanage\")).collect()[0].asDict(\"Meanage\")[\"Meanage\"]\n",
        "mean_age"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "5AjCIu67slpX",
        "outputId": "51d6b658-a2f5-41f4-a4af-c2452311ef6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+---+\n",
            "|       Name|Age|\n",
            "+-----------+---+\n",
            "|Swaminathan| 24|\n",
            "|      Peter| 39|\n",
            "|        Sam| 54|\n",
            "|      Henry| 39|\n",
            "+-----------+---+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#This will specifically fill all the null values of Age column with it's mean(average of age).\n",
        "df.na.fill(value=int(mean_age),subset=\"Age\").show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
